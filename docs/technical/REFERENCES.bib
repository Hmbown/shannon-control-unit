@article{rissanen1978modeling,
  title={Modeling by shortest data description},
  author={Rissanen, Jorma},
  journal={Automatica},
  volume={14},
  number={5},
  pages={465--471},
  year={1978},
  publisher={Elsevier}
}

@article{grunwald2007minimum,
  title={The minimum description length principle},
  author={Gr{\"u}nwald, Peter D},
  year={2007},
  publisher={MIT press}
}

@article{hinton1993keeping,
  title={Keeping the neural networks simple by minimizing the description length of the weights},
  author={Hinton, Geoffrey E and Van Camp, Drew},
  journal={Proceedings of the sixth annual conference on Computational learning theory},
  pages={5--13},
  year={1993}
}

@inproceedings{tishby2015deep,
  title={Deep learning and the information bottleneck principle},
  author={Tishby, Naftali and Zaslavsky, Noga},
  booktitle={2015 IEEE Information Theory Workshop (ITW)},
  pages={1--5},
  year={2015},
  organization={IEEE}
}

@article{achille2018information,
  title={Information dropout: Learning optimal representations through noisy computation},
  author={Achille, Alessandro and Soatto, Stefano},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={40},
  number={12},
  pages={2897--2905},
  year={2018},
  publisher={IEEE}
}

@article{blier2018description,
  title={The description length of deep learning models},
  author={Blier, L{\'e}onard and Ollivier, Yann},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@article{kingma2015adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={International Conference on Learning Representations},
  year={2015}
}

@article{you2019large,
  title={Large batch optimization for deep learning: Training bert in 76 minutes},
  author={You, Yang and Li, Jing and Reddi, Sashank and Hseu, Jonathan and Kumar, Sanjiv and Bhojanapalli, Srinadh and Song, Xiaodan and Demmel, James and Keutzer, Kurt and Hsieh, Cho-Jui},
  journal={International Conference on Learning Representations},
  year={2019}
}

@article{jaderberg2017population,
  title={Population based training of neural networks},
  author={Jaderberg, Max and Dalibard, Valentin and Osindero, Simon and Czarnecki, Wojciech M and Donahue, Jeff and Razavi, Ali and Vinyals, Oriol and Green, Tim and Dunning, Iain and Simonyan, Karen and others},
  journal={arXiv preprint arXiv:1711.09846},
  year={2017}
}

@book{khalil2002nonlinear,
  title={Nonlinear systems},
  author={Khalil, Hassan K},
  edition={3},
  year={2002},
  publisher={Prentice hall}
}

@book{sastry1989adaptive,
  title={Adaptive control: stability, convergence and robustness},
  author={Sastry, Shankar and Bodson, Marc},
  year={1989},
  publisher={Prentice-Hall, Inc.}
}

@book{borkar2008stochastic,
  title={Stochastic approximation: a dynamical systems viewpoint},
  author={Borkar, Vivek S},
  year={2008},
  publisher={Cambridge University Press}
}

@article{hu2021lora,
  title={LoRA: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={International Conference on Learning Representations},
  year={2022}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{robbins1951stochastic,
  title={A stochastic approximation method},
  author={Robbins, Herbert and Monro, Sutton},
  journal={The annals of mathematical statistics},
  pages={400--407},
  year={1951},
  publisher={JSTOR}
}

@article{polyak1992acceleration,
  title={Acceleration of stochastic approximation by averaging},
  author={Polyak, Boris T and Juditsky, Anatoli B},
  journal={SIAM journal on control and optimization},
  volume={30},
  number={4},
  pages={838--855},
  year={1992},
  publisher={SIAM}
}

@inproceedings{zhang2017understanding,
  title={Understanding deep learning requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  booktitle={International Conference on Learning Representations},
  year={2017}
}

@article{smith2018disciplined,
  title={A disciplined approach to neural network hyper-parameters: Part 1--learning rate, batch size, momentum, and weight decay},
  author={Smith, Leslie N},
  journal={arXiv preprint arXiv:1803.09820},
  year={2018}
}

@article{loshchilov2017sgdr,
  title={SGDR: Stochastic gradient descent with warm restarts},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={International Conference on Learning Representations},
  year={2017}
}

@article{astrom1995pid,
  title={PID controllers: theory, design, and tuning},
  author={{\AA}str{\"o}m, Karl Johan and H{\"a}gglund, Tore},
  year={1995},
  publisher={Instrument society of America Research Triangle Park, NC}
}

@article{skogestad2003simple,
  title={Simple analytic rules for model reduction and PID controller tuning},
  author={Skogestad, Sigurd},
  journal={Journal of process control},
  volume={13},
  number={4},
  pages={291--309},
  year={2003},
  publisher={Elsevier}
}

@article{hochreiter1997flat,
  title={Flat minima},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={1},
  pages={1--42},
  year={1997},
  publisher={MIT Press}
}

@article{keskar2016large,
  title={On large-batch training for deep learning: Generalization gap and sharp minima},
  author={Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  journal={International Conference on Learning Representations},
  year={2017}
}

@article{smith2017bayesian,
  title={A Bayesian perspective on generalization and stochastic gradient descent},
  author={Smith, Samuel L and Le, Quoc V},
  journal={International Conference on Learning Representations},
  year={2018}
}

@article{maddox2019simple,
  title={A simple baseline for bayesian uncertainty in deep learning},
  author={Maddox, Wesley J and Izmailov, Pavel and Garipov, Timur and Vetrov, Dmitry P and Wilson, Andrew Gordon},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{fort2019deep,
  title={Deep ensembles: A loss landscape perspective},
  author={Fort, Stanislav and Hu, Huiyi and Lakshminarayanan, Balaji},
  journal={arXiv preprint arXiv:1912.02757},
  year={2019}
}

@inproceedings{li2018visualizing,
  title={Visualizing the loss landscape of neural nets},
  author={Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  booktitle={Advances in neural information processing systems},
  year={2018}
}

@article{neyshabur2017exploring,
  title={Exploring generalization in deep learning},
  author={Neyshabur, Behnam and Bhojanapalli, Srinadh and McAllester, David and Srebro, Nati},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{jiang2019fantastic,
  title={Fantastic generalization measures and where to find them},
  author={Jiang, Yiding and Krishnan, Dilip and Mobahi, Hossein and Bengio, Samy},
  journal={International Conference on Learning Representations},
  year={2020}
}

@article{dziugaite2017computing,
  title={Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data},
  author={Dziugaite, Gintare Karolina and Roy, Daniel M},
  journal={Uncertainty in Artificial Intelligence},
  year={2017}
}

@article{snoek2012practical,
  title={Practical bayesian optimization of machine learning algorithms},
  author={Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
  journal={Advances in neural information processing systems},
  volume={25},
  year={2012}
}

@article{bergstra2012random,
  title={Random search for hyper-parameter optimization},
  author={Bergstra, James and Bengio, Yoshua},
  journal={Journal of machine learning research},
  volume={13},
  number={2},
  year={2012}
}

@article{li2017hyperband,
  title={Hyperband: A novel bandit-based approach to hyperparameter optimization},
  author={Li, Lisha and Jamieson, Kevin and DeSalvo, Giulia and Rostamizadeh, Afshin and Talwalkar, Ameet},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={6765--6816},
  year={2017},
  publisher={JMLR. org}
}

@article{falkner2018bohb,
  title={BOHB: Robust and efficient hyperparameter optimization at scale},
  author={Falkner, Stefan and Klein, Aaron and Hutter, Frank},
  journal={International Conference on Machine Learning},
  pages={1437--1446},
  year={2018},
  organization={PMLR}
}

@article{golovin2017google,
  title={Google vizier: A service for black-box optimization},
  author={Golovin, Daniel and Solnik, Benjamin and Moitra, Subhodeep and Kochanski, Greg and Karro, John and Sculley, D},
  journal={Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining},
  pages={1487--1495},
  year={2017}
}